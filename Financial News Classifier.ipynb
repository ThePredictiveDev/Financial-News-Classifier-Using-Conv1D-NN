{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e457c90",
   "metadata": {},
   "source": [
    "# Financial News Classifier Using Conv1D with a Trainable Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700d7232",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ee7ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from newsapi import NewsApiClient\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd854273",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec3669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_financial_phrase_bank(data_dir, encoding='utf-8'):\n",
    "    sentences = []\n",
    "    sentiments = []\n",
    "\n",
    "    # Iterate over all files in the directory\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.startswith(\"Sentences_\"):\n",
    "            filepath = os.path.join(data_dir, filename)\n",
    "            try:\n",
    "                with open(filepath, 'r', encoding=encoding) as file:\n",
    "                    for line in file:\n",
    "                        line = line.strip()\n",
    "                        if line:\n",
    "                            # Split the sentence and sentiment label\n",
    "                            sentence, sentiment = line.rsplit('@', 1)\n",
    "                            sentences.append(sentence.strip())\n",
    "                            sentiments.append(sentiment.strip())\n",
    "            except UnicodeDecodeError as e:\n",
    "                print(f\"Error decoding {filename} with encoding {encoding}: {e}\")\n",
    "                continue  # Skip files that cause decoding errors\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'sentence': sentences,\n",
    "        'sentiment': sentiments\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "# Usage with different encodings\n",
    "data_dir = \"Your Download Directory/FinancialPhraseBank-v1.0\"\n",
    "\n",
    "# List of encodings to try\n",
    "encodings = ['utf-8', 'latin1', 'cp1252', 'iso-8859-1']\n",
    "\n",
    "for enc in encodings:\n",
    "    print(f\"Trying encoding: {enc}\")\n",
    "    df = load_financial_phrase_bank(data_dir, encoding=enc)\n",
    "    if not df.empty:\n",
    "        print(f\"Successfully loaded data with encoding: {enc}\")\n",
    "        break  # Exit the loop once successful\n",
    "else:\n",
    "    print(\"Failed to load data with tried encodings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfb2d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping sentiment labels to numerical values\n",
    "label_mapping = {\"positive\": 1, \"neutral\": 0, \"negative\": -1}\n",
    "df['sentiment'] = df['sentiment'].map(label_mapping)\n",
    "\n",
    "# Verify the mapping\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c36d74",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4f1bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "df_train, df_test = train_test_split(df, test_size=0.1, random_state=RANDOM_SEED)\n",
    "\n",
    "print(f\"Training samples: {len(df_train)}, Testing samples: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebb7734",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782a349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the sentiment labels (e.g., -1 -> 0, 0 -> 1, 1 -> 2)\n",
    "label_encoder = LabelEncoder()\n",
    "df_train['sentiment'] = label_encoder.fit_transform(df_train['sentiment'])\n",
    "df_test['sentiment'] = label_encoder.transform(df_test['sentiment'])\n",
    "\n",
    "# Define the TextVectorization layer\n",
    "max_features = 20000  # Maximum vocabulary size\n",
    "sequence_length = 128  # Maximum sequence length\n",
    "\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length\n",
    ")\n",
    "\n",
    "# Adapt the vectorization layer on the training data\n",
    "vectorize_layer.adapt(df_train['sentence'].values)\n",
    "\n",
    "# Vectorize the sentences\n",
    "train_inputs = vectorize_layer(df_train['sentence'].values)\n",
    "test_inputs = vectorize_layer(df_test['sentence'].values)\n",
    "\n",
    "train_labels = tf.convert_to_tensor(df_train['sentiment'].values)\n",
    "test_labels = tf.convert_to_tensor(df_test['sentiment'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcaf3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow datasets from the vectorized inputs and labels\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs, train_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_inputs, test_labels))\n",
    "\n",
    "# Shuffle, batch, and prefetch the datasets for training\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b436686",
   "metadata": {},
   "source": [
    "## Model Creation, Compilation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae36977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# Define the model using a trainable embedding layer\n",
    "class SentimentClassifier(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.embedding = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "        self.conv = layers.Conv1D(128, 5, activation='relu')\n",
    "        self.global_pool = layers.GlobalMaxPooling1D()\n",
    "        self.dropout = layers.Dropout(0.5)\n",
    "        self.classifier = layers.Dense(n_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.conv(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Initialize the model with a trainable embedding layer\n",
    "vocab_size = len(vectorize_layer.get_vocabulary())\n",
    "embedding_dim = 128\n",
    "n_classes = len(label_encoder.classes_)\n",
    "\n",
    "classifier_model = SentimentClassifier(vocab_size, embedding_dim, n_classes)\n",
    "\n",
    "# Compile the model\n",
    "classifier_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "                         loss='sparse_categorical_crossentropy',\n",
    "                         metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = classifier_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=20,\n",
    "    validation_data=test_dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec07835",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23da9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "loss, accuracy = classifier_model.evaluate(test_dataset)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5eb80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred_probs = classifier_model.predict(test_dataset)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Get the true labels\n",
    "y_true = np.concatenate([y for x, y in test_dataset], axis=0)\n",
    "\n",
    "# Convert integer class labels to strings\n",
    "target_names = [str(cls) for cls in label_encoder.classes_]\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a06f18a",
   "metadata": {},
   "source": [
    "## Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce1b9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.save('sentiment_classifier_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4809ede8",
   "metadata": {},
   "source": [
    "## Use the Below Class to Fetch Real Time News and Test Accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60898c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsFetcher:\n",
    "    def __init__(self, api_key):\n",
    "        self.newsapi = NewsApiClient(api_key=\"c3cee8f6f03c4788b3b68bc89cdbae42\")\n",
    "\n",
    "    def fetch_latest_news(self, query='stock market'):\n",
    "        all_articles = self.newsapi.get_everything(q=query,\n",
    "                                                   language='en',\n",
    "                                                   sort_by='publishedAt',\n",
    "                                                   page_size=5)\n",
    "        headlines = [article['title'] for article in all_articles['articles']]\n",
    "        return headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5c8dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_fetcher = NewsFetcher(api_key=\"your_api_key\")\n",
    "headlines = news_fetcher.fetch_latest_news(query='stock market')\n",
    "\n",
    "# Initialize the trader\n",
    "trader = SentimentAnalysisTrader(model=classifier_model, vectorize_layer=vectorize_layer)\n",
    "\n",
    "# Predict sentiment and decide trade action\n",
    "for headline in headlines:\n",
    "    sentiment_score = trader.predict_sentiment([headline])\n",
    "    action = trader.decide_trade_action(sentiment_score[0])\n",
    "    print(f\"Headline: {headline}\\nSentiment: {sentiment_score[0]} -> Action: {action}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
